# CDial-GPT

* æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªå¤§è§„æ¨¡ä¸­æ–‡å¯¹è¯æ•°æ®é›†ï¼Œå¹¶æä¾›äº†åœ¨æ­¤æ•°æ®é›†ä¸Šçš„ä¸­æ–‡å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸­æ–‡GPTæ¨¡å‹ï¼‰ï¼Œæ›´å¤šä¿¡æ¯å¯å‚è€ƒæˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2008.03946)ã€‚

* æœ¬é¡¹ç›®ä»£ç ä¿®æ”¹è‡ª[TransferTransfo](https://github.com/huggingface/transfer-learning-conv-ai)ï¼Œä½¿ç”¨äº†HuggingFace Pytorchç‰ˆçš„[Transformers](https://github.com/huggingface/transformers)åº“, å¯ç”¨äºé¢„è®­ç»ƒä¸å¾®è°ƒã€‚

## ç›®å½•
* <a href="#Dataset-zh">æ•°æ®é›†æ¦‚å†µ</a>
* <a href="#Pre-training-zh">é¢„è®­ç»ƒæ¨¡å‹æ¦‚å†µ</a>
* <a href="#Evaluation-zh">è¯„æµ‹ç»“æœ</a>

## News
- 2021-01-09: å®éªŒå®¤å‡ºç‰ˆæ–°ä¹¦[ã€Šç°ä»£è‡ªç„¶è¯­è¨€ç”Ÿæˆã€‹](https://github.com/thu-coai/NLG_book)ï¼Œæ¬¢è¿å¤§å®¶é˜…è¯»è´­ä¹°ã€‚
- 2020-11-20: å®éªŒå®¤é¢„è®­ç»ƒæ¨¡å‹æ–°å·¥ä½œ[SentiLARE](https://github.com/thu-coai/SentiLARE)ã€‚æœ¬å·¥ä½œå°†è¯çº§åˆ«çš„è¯­è¨€å­¦çŸ¥è¯†ï¼ˆåŒ…æ‹¬è¯æ€§å’Œè¯çš„æƒ…æ„Ÿææ€§ï¼‰å¼•å…¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ï¼Œæå‡ºäº†ä¸€ç§é€‚ç”¨äºæƒ…æ„Ÿåˆ†æä»»åŠ¡çš„è¯­è¨€è¡¨ç¤ºæ¨¡å‹SentiLAREï¼Œæ¬¢è¿å¤§å®¶ä½¿ç”¨ã€‚
- 2020-10-18: æˆ‘ä»¬çš„è®ºæ–‡ã€ŠA Large-Scale Chinese Short-Text Conversation Datasetã€‹è·å¾—äº†NLPCC2020 Best Student Paper Awardã€‚ ğŸ‰ ğŸ‰ ğŸ‰ 
- - 2020-09-08: æ„Ÿè°¢[@xiejiachen](https://github.com/xiejiachen)æ‰€æä¾›çš„[å¯è§†åŒ–Webç•Œé¢](https://github.com/thu-coai/CDial-GPT/tree/master/contrib/dash_app)ã€‚
- 2020-09-02: å¯ç”¨[bert4keras](https://github.com/bojone/bert4keras)åŠ è½½[TFç‰ˆæœ¬çš„CDial-GPTæ¨¡å‹](https://github.com/bojone/CDial-GPT-tf)ï¼Œæ„Ÿè°¢è‹å‰‘æ—[@bojone](https://github.com/bojone)æä¾›ä»£ç ã€‚

## <a name="#Dataset-zh">æ•°æ®é›†æ¦‚å†µ</a>
æˆ‘ä»¬æ‰€æä¾›çš„æ•°æ®é›†LCCC(Large-scale Cleaned Chinese Conversation)ä¸»è¦åŒ…å«ä¸¤éƒ¨åˆ†:
[LCCC-base](https://cloud.tsinghua.edu.cn/f/f131a4d259184566a29c/) å’Œ
[LCCC-large](https://cloud.tsinghua.edu.cn/f/8424e7b9454c4e628c24/).
æˆ‘ä»¬è®¾è®¡äº†ä¸€å¥—ä¸¥æ ¼çš„æ•°æ®è¿‡æ»¤æµç¨‹æ¥ç¡®ä¿è¯¥æ•°æ®é›†ä¸­å¯¹è¯æ•°æ®çš„è´¨é‡ã€‚
è¿™ä¸€æ•°æ®è¿‡æ»¤æµç¨‹ä¸­åŒ…æ‹¬ä¸€ç³»åˆ—æ‰‹å·¥è§„åˆ™ä»¥åŠè‹¥å¹²åŸºäºæœºå™¨å­¦ä¹ ç®—æ³•æ‰€æ„å»ºçš„åˆ†ç±»å™¨ã€‚
æˆ‘ä»¬æ‰€è¿‡æ»¤æ‰çš„å™ªå£°åŒ…æ‹¬ï¼šè„å­—è„è¯ã€ç‰¹æ®Šå­—ç¬¦ã€é¢œè¡¨æƒ…ã€è¯­æ³•ä¸é€šçš„è¯­å¥ã€ä¸Šä¸‹æ–‡ä¸ç›¸å…³çš„å¯¹è¯ç­‰ã€‚

è¯¥æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚
å…¶ä¸­ï¼Œæˆ‘ä»¬å°†ä»…åŒ…å«ä¸¤ä¸ªè¯­å¥çš„å¯¹è¯ç§°ä¸ºâ€œå•è½®å¯¹è¯â€ï¼Œæˆ‘ä»¬å°†åŒ…å«ä¸¤ä¸ªä»¥ä¸Šè¯­å¥çš„å¯¹è¯ç§°ä¸ºâ€œå¤šè½®å¯¹è¯â€ã€‚
ç»Ÿè®¡è¯è¡¨å¤§å°æ—¶ä½¿ç”¨ [Jieba](https://github.com/fxsjy/jieba) åˆ†è¯ã€‚

| [LCCC-base](https://cloud.tsinghua.edu.cn/f/f131a4d259184566a29c/) | å•è½®å¯¹è¯ | å¤šè½®å¯¹è¯  |
| :----------------------------------------------------------- | :--------- | :---------  |
| æ€»å¯¹è¯è½®æ¬¡                                                    |  3,354,382 |  3,466,607  |
| æ€»å¯¹è¯è¯­å¥                                                    |  6,708,554 | 13,365,268  |
| æ€»å­—ç¬¦æ•°                                                      | 68,559,727 | 163,690,614 |
| è¯è¡¨å¤§å°                                                      |   372,063  |   666,931   |
| å¯¹è¯è¯­å¥çš„è¯„ä»·è¯æ•°                                             |    6.79    |    8.32     |
| æ¯è½®å¯¹è¯çš„å¹³å‡è¯­å¥æ•°                                           |      2     |    3.86     |

è¯·æ³¨æ„ï¼Œ LCCC-base æ•°æ®é›†çš„æ¸…æ´—è¿‡ç¨‹æ¯” LCCC-large æ›´ä¸ºä¸¥æ ¼ï¼Œå› æ­¤å…¶è§„æ¨¡ä¹Ÿæ›´å°ã€‚

| [LCCC-large](https://cloud.tsinghua.edu.cn/f/8424e7b9454c4e628c24/) | å•è½®å¯¹è¯ | å¤šè½®å¯¹è¯  |
| :----------------------------------------------------------- | :---------  | :---------  |
| æ€»å¯¹è¯è½®æ¬¡                                                    |  7,273,804  |  4,733,955  |
| æ€»å¯¹è¯è¯­å¥                                                    | 14,547,608  | 18,341,167  |
| æ€»å­—ç¬¦æ•°                                                      | 162,301,556 | 217,776,649 |
| è¯è¡¨å¤§å°                                                      |   662,514   |   690,027   |
| å¯¹è¯è¯­å¥çš„è¯„ä»·è¯æ•°                                             |    7.45     |    8.14     |
| æ¯è½®å¯¹è¯çš„å¹³å‡è¯­å¥æ•°                                           |      2      |    3.87     |

LCCC-base æ•°æ®é›†ä¸­çš„åŸå§‹å¯¹è¯æ•°æ®æ¥è‡ªäºå¾®åšå¯¹è¯ï¼ŒLCCC-large æ•°æ®é›†ä¸­çš„åŸå§‹å¯¹è¯æ•°æ®åœ¨è¿™äº›å¾®åšå¯¹è¯çš„åŸºç¡€ä¸Šèåˆäº†å…¶ä»–å¼€æºå¯¹è¯æ•°æ®é›†ï¼š

| æ•°æ®é›†                              | æ€»å¯¹è¯è½®æ¬¡  | å¯¹è¯ç¤ºä¾‹                               |
| :---------------------------------- | :-------: | :---------------------------------- |
| Weibo Corpus                      | 79M | Q:ç«é”…æˆ‘åœ¨é‡åº†æˆéƒ½åƒäº†ä¸ƒå…«é¡¿ç«é”… A: å“ˆå“ˆå“ˆå“ˆï¼é‚£æˆ‘çš„å˜´å·´ å¯èƒ½è¦çƒ‚æ‰ï¼ |
| [PTT Gossiping Corpus](https://github.com/zake7749/Gossiping-Chinese-Corpus) | 0.4M | Q:ä¸ºä»€ä¹ˆä¹¡æ°‘æ€»æ˜¯æ¬ºè´Ÿå›½é«˜ä¸­ç”Ÿå‘¢QQ A:å¦‚æœä»¥ä¸ºé€‰å¥½ç§‘ç³»å°±ä¼šå˜æˆæ¯”å°”ç›–å…¹é‚£ä¸å¦‚é€€å­¦å§ |
| [Subtitle Corpus](https://github.com/skdjfla/dgk_lost_conv) | 2.74M | Q:äº¬æˆé‡Œå¤´çš„äººéƒ½æ˜¯ä¸è‡ªç”±çš„ A:ä»–ä»¬è®©äººæ‹¿ç¬¼å­ç»™å¥—èµ·æ¥äº†äº† |
| [Xiaohuangji Corpus](https://github.com/skdjfla/dgk_lost_conv) | 0.45M | Q:ä½ è°ˆè¿‡æ‹çˆ±ä¹ˆ A:è°ˆè¿‡ï¼Œå“ï¼Œåˆ«æäº†ï¼Œä¼¤å¿ƒ.. |
| [Tieba Corpus](https://github.com/codemayq/chinese_chatbot_corpus) | 2.32M | Q:å‰æ’ï¼Œé²è¿·ä»¬éƒ½èµ·åºŠäº†å§ A:æ ‡é¢˜è¯´åŠ©æ”»ï¼Œä½†æ˜¯çœ‹äº†é‚£çƒï¼ŒçœŸæ˜¯æ´»ç”Ÿç”Ÿçš„è®½åˆºäº† |
| [Qingyun Corpus](https://github.com/codemayq/chinese_chatbot_corpus) | 0.1M | Q:çœ‹æ¥ä½ å¾ˆçˆ±é’± A:å™¢æ˜¯å—ï¼Ÿé‚£ä¹ˆä½ ä¹Ÿå·®ä¸å¤šäº† |
| [Douban Conversation Corpus](https://github.com/MarkWuNLP/MultiTurnResponseSelection) | 0.5M | Q:çœ‹åŸç‰ˆè‹±æ–‡ç”µå½±å­¦çº¯æ­£è‹±è¯­ A:å¤§çˆ±è€å‹è®°åå¤çœ‹äº†å¥½å¤šæ¬¡ äº† Q:ä¸€æ ·å…‰ç›˜éƒ½å¿«è¢«æˆ‘çœ‹èŠ±äº† A:é‚£ä½ ç°åœ¨çš„è‹±è¯­åº”è¯¥ä¸é”™äº† |
| [E-commerical Conversation Corpus](https://github.com/cooelf/DeepUtteranceAggregation) | 0.5M | Q:è¿™ä¸ªä¼šä¸ä¼šèšåˆ’ç®— A:æš‚æ—¶æ²¡æœ‰å“¦ Q:åæœŸä¼šä¸ä¼šæœ‰ A:ä¸ä¸€å®šå“¦äº²å¤šå¤šå…³æ³¨æˆ‘ä»¬å“¦ |
| [Chinese Chat Corpus](https://github.com/yangjianxin1/GPT2-chitchat) | 0.5M | Q: æˆ‘ä»Šå¤©è…¿éƒ½åºŸäº†ï¼Œä½ ä»¬è¿‡èŠ‚ï¼Œæˆ‘æ¬ç – A: è¾›è‹¦å•Šï¼Œåœ£è¯èŠ‚è¿˜å»èµšå¤§é’±äº†åŠ æ²¹ Q: æ¯•ç«Ÿæ˜¯æ²¡ç”·æœ‹å‹çš„äººï¼Œä»€ä¹ˆèŠ‚éƒ½æ˜¯ä¸€æ ·çš„ |

## <a name="#Pre-training-en">é¢„è®­ç»ƒæ¨¡å‹æ¦‚å†µ</a>
### æ¨¡å‹  
æˆ‘ä»¬åŒæ—¶æä¾›äº†ä¸€ç³»åˆ—ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸­æ–‡GPTæ¨¡å‹ï¼‰ï¼Œè¿™äº›æ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤æ­¥ï¼Œé¦–å…ˆåœ¨ä¸€ä¸ªä¸­æ–‡å°è¯´æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨LCCCæ•°æ®é›†ä¸Šé¢„è®­ç»ƒã€‚

æˆ‘ä»¬æ²¿ç”¨äº† [TransferTransfo](https://arxiv.org/abs/1901.08149) ä¸­çš„æ•°æ®é¢„å¤„ç†è®¾å®šï¼Œæ—¢å°†æ‰€æœ‰çš„å¯¹è¯å†å²æ‹¼æ¥ä¸ºä¸€ä¸ªå¥å­ï¼Œç„¶åä½¿ç”¨è¿™ä¸ªå¥å­ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œé¢„æµ‹å¯¹è¯å›å¤ã€‚æˆ‘ä»¬æ¨¡å‹çš„è¾“å…¥é™¤äº†å„ä¸ªè¯çš„å‘é‡è¡¨ç¤ºå¤–ï¼Œè¿˜åŒ…æ‹¬å‘è¯äººå‘é‡è¡¨ç¤ºå’Œä½ç½®å‘é‡è¡¨ç¤ºã€‚

![æ¨¡å‹è¾“å…¥](figures/inputs.png)

| é¢„è®­ç»ƒæ¨¡å‹        | å‚æ•°æ•°é‡ | é¢„è®­ç»ƒæ‰€ä½¿ç”¨æ•°æ®   | æè¿°                                       |
|---------------------| ------ |--------------------------|-------------------------------------------------- |
| GPT<sub>Novel</sub>                 | 95.5M | ä¸­æ–‡å°è¯´æ•°æ®| åŸºäºä¸­æ–‡å°è¯´æ•°æ®æ‰€æ„å»ºä¸­æ–‡é¢„è®­ç»ƒGPTæ¨¡å‹ ï¼ˆè¯¥å°è¯´æ•°æ®ä¸­å…±åŒ…æ‹¬1.3Bä¸ªå­—ï¼‰  |
| [CDial-GPT<sub>LCCC-base</sub>](https://huggingface.co/thu-coai/CDial-GPT_LCCC-base)   | 95.5M | LCCC-base  | åœ¨GPT<sub>Novel</sub>çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ LCCC-base è®­ç»ƒå¾—åˆ°çš„ä¸­æ–‡é¢„è®­ç»ƒGPTæ¨¡å‹   |
| [CDial-GPT2<sub>LCCC-base</sub>](https://huggingface.co/thu-coai/CDial-GPT2_LCCC-base) | 95.5M | LCCC-base  | åœ¨GPT<sub>Novel</sub>çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ LCCC-base è®­ç»ƒå¾—åˆ°çš„ä¸­æ–‡é¢„è®­ç»ƒGPT2æ¨¡å‹  |
| [CDial-GPT<sub>LCCC-large</sub>](https://huggingface.co/thu-coai/CDial-GPT_LCCC-large) | 95.5M | LCCC-large | åœ¨GPT<sub>Novel</sub>çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ LCCC-large è®­ç»ƒå¾—åˆ°çš„ä¸­æ–‡é¢„è®­ç»ƒGPTæ¨¡å‹  |

### å®‰è£…  
ä»æºä»£ç ç›´æ¥å®‰è£…ï¼š

    git clone https://github.com/thu-coai/CDial-GPT.git
    cd CDial-GPT
    pip install -r requirements.txt 
    
### å¿«é€Ÿå¼€å§‹
Step 1: å‡†å¤‡é¢„è®­ç»ƒæ¨¡å‹å’Œ fine-tuning æ‰€éœ€ä½¿ç”¨çš„æ•°æ®é›†(å¦‚ [STC dataset](https://arxiv.org/abs/1503.02364) æˆ–è¯¥é¡¹ç›®ç›®å½•ä¸­çš„toyæ•°æ® "data/toy_data.json", è¯·æ³¨æ„å¦‚æ•°æ®ä¸­åŒ…å«è‹±æ–‡éœ€æŒ‰å­—æ¯åˆ†å‰²å¦‚ï¼šh e l l o)
    
    wget https://cloud.tsinghua.edu.cn/f/372be4a9994b4124810e/?dl=1 -O STC-corpus.zip  # ä¸‹è½½ STC æ•°æ®é›†å¹¶å°†å…¶è§£å‹è‡³ "data_path" ç›®å½• (å¦‚æœå¾®è°ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ä¸º STC)
    git lfs install
    git clone https://huggingface.co/thu-coai/CDial-GPT_LCCC-large  # æ‚¨å¯è‡ªè¡Œä¸‹è½½æ¨¡å‹æˆ–è€…OpenAIGPTLMHeadModel.from_pretrained("thu-coai/CDial-GPT_LCCC-large")
  
Step 2: è®­ç»ƒæ¨¡å‹

    python train.py --pretrained --model_checkpoint thu-coai/CDial-GPT_LCCC-large --data_path data/STC.json --scheduler linear  # ä½¿ç”¨å•ä¸ªGPUè¿›è¡Œè®­ç»ƒ

æˆ–è€…

    python -m torch.distributed.launch --nproc_per_node=8 train.py --pretrained --model_checkpoint thu-coai/CDial-GPT_LCCC-large --data_path data/STC.json --scheduler linear  # ä»¥åˆ†å¸ƒå¼çš„æ–¹å¼åœ¨8å—GPUä¸Šè®­ç»ƒ

æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬ä¸­è¿˜æä¾›äº† ``train_path`` å‚æ•°ï¼Œç”¨æˆ·å¯ä½¿ç”¨è¯¥å‚æ•°ä»¥åˆ‡ç‰‡åœ°å½¢å¼è¯»å–çº¯æ–‡æœ¬æ–‡ä»¶ã€‚å¦‚æœæ‚¨æ‰€ä½¿ç”¨çš„çš„ç³»ç»Ÿä¸­å†…å­˜æœ‰é™ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨è¯¥å‚æ•°è¯»å…¥è®­ç»ƒæ•°æ®ã€‚
å¦‚æœæ‚¨ä½¿ç”¨ ``train_path`` åˆ™éœ€è¦å°† ``data_path`` ç½®ç©ºã€‚ 

Step 3: ç”Ÿæˆæ–‡æœ¬

    # YOUR_MODEL_PATH: ä½ è¦ä½¿ç”¨çš„æ¨¡å‹çš„è·¯å¾„ï¼Œæ¯æ¬¡å¾®è°ƒåçš„æ¨¡å‹ç›®å½•ä¿å­˜åœ¨./runs/ä¸­
    python infer.py --model_checkpoint YOUR_MODEL_PATH --datapath data/STC_test.json --out_path STC_result.txt  # åœ¨æµ‹è¯•æ•°æ®ä¸Šç”Ÿæˆå›å¤
    python interact.py --model_checkpoint YOUR_MODEL_PATH  # åœ¨å‘½ä»¤è¡Œä¸­ä¸æ¨¡å‹è¿›è¡Œäº¤äº’

è®­ç»ƒè„šæœ¬å‚æ•°

| å‚æ•°  | ç±»å‹     | é»˜è®¤å€¼  | æè¿° |
| :---- | :---------- | :----- | :------- |
| model_checkpoint | str | "" | Path or URL of model files (Directory of pre-training model and config/vocab files) |
| pretrained  | bool | False | If False, then train the model from scratch |
| data_path | str | "" | Path of the dataset |
| dataset_cache | str | default="dataset_cache" | Path or url of the dataset cache |
| train_path | str | "" | Path of the training set for distributed dataset |
| valid_path | str | "" | Path of the validation set for distributed dataset |
| log_file | str | "" | Output logs to a file under this path |
| num_workers | int | 1 | Number of subprocesses for data loading |
| n_epochs | int | 70 | Number of training epochs |
| train_batch_size | int | 8 | Batch size for training |
| valid_batch_size | int | 8 | Batch size for validation |
| max_history | int | 15 | Number of previous exchanges to keep in history |
| scheduler | str | "noam" | Method of optimizer |
| n_emd | int | 768 | Number of n_emd in config file (for noam) |
| eval_before_start | bool | False | If true, start evaluation before training |
| warmup_steps | int | 5000 | Warm up steps |
| valid_steps | int | 0 | Perform validation every X steps, if is not 0 |
| gradient_accumulation_steps | int | 64 | Accumulate gradients on several steps |
| max_norm | float | 1.0 | Clipping gradient norm |
| device | str | "cuda" if torch.cuda.is_available() else "cpu" | Device (cuda or cpu) |
| fp16 | str | "" | Set to O0, O1, O2 or O3 for fp16 training (see apex documentation) |
| local_rank | int | -1 | Local rank for distributed training (-1: not distributed) |

## <a name="#Evaluation-en">è¯„æµ‹ç»“æœ</a> 
æˆ‘ä»¬è¯„æµ‹äº†ä½¿ç”¨
[STC dataset](https://cloud.tsinghua.edu.cn/f/372be4a9994b4124810e/)
å¾®è°ƒåçš„å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚
æ‰€æœ‰çš„å›å¤å‡ä½¿ç”¨ [Nucleus Sampling](https://arxiv.org/abs/1904.09751) çš„æ–¹æ³•é‡‡æ ·å¾—åˆ° (p=0.9, temperature=0.7)ã€‚

#### è‡ªåŠ¨è¯„ä»·æŒ‡æ ‡

| æ¨¡å‹  | æ¨¡å‹å¤§å° | PPL  | BLEU-2 | BLEU-4 | Dist-1 | Dist-2 | Greedy Matching | Embedding Average |
| :------ | :------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: |
| Attn-Seq2seq | 73M | 34.20 | 3.93 | 0.90 | 8.5 | 11.91 | 65.84 | 83.38 |
| Transformer | 113M | 22.10 | 6.72 | 3.14 | 8.8 | 13.97 | 66.06 | 83.55 |
| GPT2-chitchat | 88M | - | 2.28 | 0.54 | 10.3 | 16.25 | 61.54 | 78.94 |
| GPT<sub>Novel</sub> | 95.5M | 21.27 | 5.96 | 2.71 | 8.0 | 11.72 | 66.12 | 83.34 |
| GPT<sub>LCCC-base</sub> | 95.5M | 18.38 | 6.48 | 3.08 | 8.3 | 12.68 | 66.21 | 83.54 |
| GPT2<sub>LCCC-base</sub> | 95.5M | 22.76 | 5.69 | 2.50 | 7.7 | 10.87 | 66.24 | 83.46 |
| GPT<sub>LCCC-large</sub> | 95.5M | 18.23 | 6.63 | 3.20 | 8.3 | 12.71 | 66.25 | 83.63 |

#### äººå·¥è¯„ä»·æŒ‡æ ‡

æˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹é‡‡æ ·äº†200ä¸ªå›å¤ï¼Œå¹¶åœ¨å¤–åŒ…å¹³å°ä¸Šé‚€è¯·äº†3ä½æ ‡æ³¨å‘˜å¯¹è¿™äº›å›å¤è¿›è¡Œäººå·¥è¯„æµ‹ï¼Œè¯„æµ‹ç»´åº¦ä¸ºå¯¹è¯æµç•…æ€§ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§å’Œå›å¤å¤šæ ·æ€§ã€‚æ¯ä¸ªç»´åº¦çš„æ‰“åˆ†èŒƒå›´ä¸º 2/1/0ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒæˆ‘ä»¬çš„ [è®ºæ–‡](https://arxiv.org/abs/2008.03946)ã€‚

| æ¨¡å‹  | +2      | +1      | +0      | Kappa   |
| :----- | :-----: | :-----: | :-----: | :-----: |
| Attn-Seq2Seq | 27.1% | 21.4% | 51.4% | 0.4544 |
| Transformer | 42.4% | 23.6% | 34.0% | 0.4429 |
| GPT2-chitchat | 24.3% | 19,1% | 56.6% | 0.3941 |
| GPT<sub>LCCC-base</sub> | 46.7% | 21.8% | 31.5% | 0.3954 |
| GPT<sub>LCCC-large</sub> | 48.6% | 24.5% | 27.0% | 0.4203 |

#### ä¸è‰¯å›å¤ç»Ÿè®¡

æˆ‘ä»¬éšæœºé‡‡æ ·äº†1000ä¸ªå¯¹è¯å›å¤ï¼Œå¹¶ç»Ÿè®¡äº†è¿™äº›å›å¤ä¸­çš„ä¸è‰¯å›å¤ï¼š

| åŒ…å«é‡å¤è¯­å¥ | åŒ…å«è„å­— | å…¶ä»–ä¸è‰¯å›å¤ |
| :----------: | :-----: | :-------: |
| 23 | 9 | 2 |

## GPT<sub>LCCC-large</sub>æ¨¡å‹çš„ç”Ÿæˆç¤ºä¾‹

æˆ‘ä»¬åœ¨STCæ•°æ®é›†ä¸Šå¾®è°ƒäº†GPT<sub>LCCC-large</sub>æ¨¡å‹çš„å‚æ•°ï¼Œå¹¶ä½¿ç”¨è¿™ä¸€å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆäº†å¦‚ä¸‹å¯¹è¯ç¤ºä¾‹ï¼š

| Post | Response |
| :----- | :---------- |
| è¿™ å±… ç„¶ æ˜¯ åŒ— äº¬ ä¸ æ˜¯ å“ˆ å°” æ»¨ ã€‚ | å“ˆ å°” æ»¨ çš„ å¤© æ°” å¥½ åƒ æ¯” åŒ— äº¬ å¥½ ç‚¹ ï¼Œ æˆ‘ è¿™ é‡Œ å°± æ˜¯ å° é›¨ ï¼Œ ç™½ å¤© éƒ½ æ˜¯ å¤§ é£ |
| è¿ è¥ å•† æ ¡ å›­ å¹¿ å‘Š å¤§ P K ï¼Œ å¤ª æ¬¢ ä¹ äº† ï¼ å“ˆ å“ˆ å“ˆ ã€‚ | æˆ‘ ä»¥ ä¸º æ˜¯ é©¬ äº‘ çš„ å¹¿ å‘Š ã€‚ |
| D e l s a S p o s a ç³– æœ è‰² ç³» åˆ— å©š çº± ï¼Œ ç²‰ è“ çº¢ ç´« ï¼Œ ä¿ çš® åˆ æ¸… æ–° ï¼Œ ä½  å–œ æ¬¢ å— ï¼Ÿ | æœ€ å ä¸€ ä»¶ å¤ª ç¾ äº† ï¼Œ ç¾ çš„ ä¸ åƒ è¯ ï¼ |

æˆ‘ä»¬è¿˜ä½¿ç”¨ä¸Šè¿°å¾®è°ƒåçš„æ¨¡å‹å¾—åˆ°äº†å¦‚ä¸‹äº¤äº’ç»“æœï¼š

| Roel | Response |
| :----- | :---------- |
| User | åˆ«çˆ±æˆ‘æ²¡ç»“æœ |
| Bot | æ²¡ ç»“ æœ çš„ çˆ± æ˜¯ å¤š ä¹ˆ çš„ ç—› ã€‚ |
| User | ä½†æ˜¯çˆ±è¿‡å°±å¯ä»¥äº† |
| Bot | æœ‰ æ—¶ å€™ ï¼Œçˆ± æƒ… å°± æ˜¯ ä¸€ åœº æ¸¸ æˆ ã€‚ |
| User | è¾“çš„æ€»æ˜¯æˆ‘ |
| Bot | æˆ‘ çˆ± ä½  ï¼Œ å°± åƒ æˆ‘ çˆ± ä½  ä¸€ æ · ã€‚ |
| User | ä½ å¯æ‹‰åˆ°å§ |
| Bot | ä½  å¯ æ‹‰ åˆ° å§ ã€‚ |
| User | åˆ«å­¦æˆ‘è¯´è¯ |
| Bot | æˆ‘ è¯´ è¯ å°± æ˜¯ ä½  ä¸ è¡Œ ã€‚ |
| User | é€¼æˆ‘åŠ¨æ‰‹æ˜¯å§ |
| Bot | ä¸ ï¼Œ æ˜¯ é€¼ ä½  åŠ¨ å£ æ˜¯ å§ |
 
## ä½œè€…
[Yida Wang](https://github.com/lemon234071), [Pei Ke](https://github.com/kepei1106), [Yinhe Zheng](https://github.com/silverriver), [Kaili Huang](https://github.com/KellyHuang63), Yong Jiang, Xiaoyan Zhu, Minlie Huang

## è‡´è°¢
[Zuoxian Ye](https://github.com/yezuoxian), Yao Wang, Yifan Pan


## å…è´£å£°æ˜
æœ¬é¡¹ç›®æ‰€æä¾›çš„ LCCC æ•°æ®é›†å’Œé¢„è®­ç»ƒå¯¹è¯æ¨¡å‹ä»…é™ç§‘ç ”ç”¨é€”ã€‚LCCCæ•°æ®é›†ä¸­çš„å¯¹è¯æ”¶é›†è‡ªä¸åŒçš„æ¥æºï¼Œè™½ç„¶æˆ‘ä»¬è®¾è®¡äº†ä¸€å¥—ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—æµç¨‹ï¼Œä½†æ˜¯æˆ‘ä»¬å¹¶ä¸ä¿è¯æ‰€æœ‰ä¸å½“å†…å®¹å‡å·²è¢«è¿‡æ»¤ã€‚è¯¥æ•°æ®ä¸­æ‰€åŒ…å«çš„æ‰€æœ‰å†…å®¹å’Œæ„è§ä¸æœ¬é¡¹ç›®ä½œè€…æ— å…³ã€‚
æœ¬é¡¹ç›®æ‰€æä¾›çš„æ¨¡å‹å’Œä»£ç ä»…ä¸ºå®Œæ•´å¯¹è¯ç³»ç»Ÿçš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ï¼Œæˆ‘ä»¬æ‰€æä¾›çš„è§£ç è„šæœ¬ä»…é™ç§‘ç ”ç”¨é€”ï¼Œä½¿ç”¨æœ¬é¡¹ç›®ä¸­çš„æ¨¡å‹å’Œè„šæœ¬æ‰€ç”Ÿæˆçš„ä¸€åˆ‡å¯¹è¯å†…å®¹ä¸æœ¬é¡¹ç›®ä½œè€…æ— å…³ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2008.03946)ï¼š

    @inproceedings{wang2020chinese,
      title={A Large-Scale Chinese Short-Text Conversation Dataset},
      author={Wang, Yida and Ke, Pei and Zheng, Yinhe and Huang, Kaili and Jiang, Yong and Zhu, Xiaoyan and Huang, Minlie},
      booktitle={NLPCC},
      year={2020},
      url={https://arxiv.org/abs/2008.03946}
    }

---

# CDial-GPT

* This project provides a large-scale cleaned Chinese **conversation dataset** and a **Chinese GPT model** pre-trained on this dataset. Please refer to our [paper](https://arxiv.org/abs/2008.03946) for more details.

* Our code used for the pre-training is adapted from the [TransferTransfo](https://github.com/huggingface/transfer-learning-conv-ai) model based on the [Transformers](https://github.com/huggingface/transformers) library. The codes used for both pre-training and fine-tuning are provided in this repository.

## Contents
* <a href="#Dataset-en">Dataset</a>
* <a href="#Pre-training-en">Pre-training</a>
* <a href="#Evaluation-en">Evaluation</a>

## <a name="#Dataset-en">Dataset</a>
We present a Large-scale Cleaned Chinese Conversation corpus (LCCC) containing:
[LCCC-base](https://cloud.tsinghua.edu.cn/f/f131a4d259184566a29c/) and
[LCCC-large](https://cloud.tsinghua.edu.cn/f/8424e7b9454c4e628c24/).
A rigorous data cleaning pipeline is designed to ensure the quality of the corpus.
This pipeline involves a set of rules and several classifier-based filters.
Noises such as offensive or sensitive words, special symbols, emojis, grammatically incorrect sentences, and incoherent conversations are filtered.

The statistic of our corpus is presented below.
Dialogues with only two utterances are regarded as "Single-turn", and dialogues with more than three utterances are regarded as "Multi-turn".
The vocabulary size is calculated in word-level, and [Jieba](https://github.com/fxsjy/jieba) is used to tokenize each utterance to words.

| [LCCC-base](https://cloud.tsinghua.edu.cn/f/f131a4d259184566a29c/) | Single-turn | Multi-turn  |
| :----------------------------------------------------------- | :--------- | :---------  |
| Sessions                                                     |  3,354,382 |  3,466,607  |
| Utterances                                                   |  6,708,554 | 13,365,268  |
| Characters                                                   | 68,559,727 | 163,690,614 |
| Vocabulary                                                   |   372,063  |   666,931   |
| Avg. words per utterance                                     |    6.79    |    8.32     |
| Avg. utterances per session                                  |      2     |    3.86     |

Note that LCCC-base is cleaned using more strict rules compared to LCCC-large. 

| [LCCC-large](https://cloud.tsinghua.edu.cn/f/8424e7b9454c4e628c24/) | Single-turn | Multi-turn  |
| :----------------------------------------------------------- | :---------  | :---------  |
| Sessions                                                     |  7,273,804  |  4,733,955  |
| Utterances                                                   | 14,547,608  | 18,341,167  |
| Characters                                                   | 162,301,556 | 217,776,649 |
| Vocabulary                                                   |   662,514   |   690,027   |
| Avg. words per utterance                                     |    7.45     |    8.14     |
| Avg. utterances per session                                  |      2      |    3.87     |

The raw dialogues for LCCC-base originate from a Weibo Corpus that we crawled from [Weibo](www.weibo.com), and the raw dialogues for LCCC-large is built by combining several conversation datasets in addition to the Weibo Corpus:

| Dataset                              | Sessions  | Sample                               |
| :---------------------------------- | :-------: | :---------------------------------- |
| Weibo Corpus                      | 79M | Q:ç«é”…æˆ‘åœ¨é‡åº†æˆéƒ½åƒäº†ä¸ƒå…«é¡¿ç«é”… A: å“ˆå“ˆå“ˆå“ˆï¼é‚£æˆ‘çš„å˜´å·´ å¯èƒ½è¦çƒ‚æ‰ï¼ |
| [PTT Gossiping Corpus](https://github.com/zake7749/Gossiping-Chinese-Corpus) | 0.4M | Q:ä¸ºä»€ä¹ˆä¹¡æ°‘æ€»æ˜¯æ¬ºè´Ÿå›½é«˜ä¸­ç”Ÿå‘¢QQ A:å¦‚æœä»¥ä¸ºé€‰å¥½ç§‘ç³»å°±ä¼šå˜æˆæ¯”å°”ç›–å…¹é‚£ä¸å¦‚é€€å­¦å§ |
| [Subtitle Corpus](https://github.com/skdjfla/dgk_lost_conv) | 2.74M | Q:äº¬æˆé‡Œå¤´çš„äººéƒ½æ˜¯ä¸è‡ªç”±çš„ A:ä»–ä»¬è®©äººæ‹¿ç¬¼å­ç»™å¥—èµ·æ¥äº†äº† |
| [Xiaohuangji Corpus](https://github.com/skdjfla/dgk_lost_conv) | 0.45M | Q:ä½ è°ˆè¿‡æ‹çˆ±ä¹ˆ A:è°ˆè¿‡ï¼Œå“ï¼Œåˆ«æäº†ï¼Œä¼¤å¿ƒ.. |
| [Tieba Corpus](https://github.com/codemayq/chinese_chatbot_corpus) | 2.32M | Q:å‰æ’ï¼Œé²è¿·ä»¬éƒ½èµ·åºŠäº†å§ A:æ ‡é¢˜è¯´åŠ©æ”»ï¼Œä½†æ˜¯çœ‹äº†é‚£çƒï¼ŒçœŸæ˜¯æ´»ç”Ÿç”Ÿçš„è®½åˆºäº† |
| [Qingyun Corpus](https://github.com/codemayq/chinese_chatbot_corpus) | 0.1M | Q:çœ‹æ¥ä½ å¾ˆçˆ±é’± A:å™¢æ˜¯å—ï¼Ÿé‚£ä¹ˆä½ ä¹Ÿå·®ä¸å¤šäº† |
| [Douban Conversation Corpus](https://github.com/MarkWuNLP/MultiTurnResponseSelection) | 0.5M | Q:çœ‹åŸç‰ˆè‹±æ–‡ç”µå½±å­¦çº¯æ­£è‹±è¯­ A:å¤§çˆ±è€å‹è®°åå¤çœ‹äº†å¥½å¤šæ¬¡ äº† Q:ä¸€æ ·å…‰ç›˜éƒ½å¿«è¢«æˆ‘çœ‹èŠ±äº† A:é‚£ä½ ç°åœ¨çš„è‹±è¯­åº”è¯¥ä¸é”™äº† |
| [E-commerical Conversation Corpus](https://github.com/cooelf/DeepUtteranceAggregation) | 0.5M | Q:è¿™ä¸ªä¼šä¸ä¼šèšåˆ’ç®— A:æš‚æ—¶æ²¡æœ‰å“¦ Q:åæœŸä¼šä¸ä¼šæœ‰ A:ä¸ä¸€å®šå“¦äº²å¤šå¤šå…³æ³¨æˆ‘ä»¬å“¦ |
| [Chinese Chat Corpus](https://github.com/yangjianxin1/GPT2-chitchat) | 0.5M | Q: æˆ‘ä»Šå¤©è…¿éƒ½åºŸäº†ï¼Œä½ ä»¬è¿‡èŠ‚ï¼Œæˆ‘æ¬ç – A: è¾›è‹¦å•Šï¼Œåœ£è¯èŠ‚è¿˜å»èµšå¤§é’±äº†åŠ æ²¹ Q: æ¯•ç«Ÿæ˜¯æ²¡ç”·æœ‹å‹çš„äººï¼Œä»€ä¹ˆèŠ‚éƒ½æ˜¯ä¸€æ ·çš„ |

## <a name="#Pre-training-en">Pre-training</a>
### Models  
We also present a series of Chinese GPT model that are first pre-trained on a Chinese novel dataset and then post-trained on our LCCC dataset.

Similar to [TransferTransfo](https://arxiv.org/abs/1901.08149), we concatenate all dialogue histories into one context sentence, and use this sentence to predict the response. The input of our model consists of word embedding, speaker embedding, and positional embedding of each word.

![Input representation](figures/inputs.png)

| Models        | Parameter Size | Pre-training Dataset   | Description                                       |
|---------------------| ------ |--------------------------|-------------------------------------------------- |
| GPT<sub>Novel</sub> | 95.5M | Chinese Novel            | A GPT model pre-trained on Chinese Novel dataset (1.3B words, note that we do not provide the detail of this model)  |
| [CDial-GPT<sub>LCCC-base</sub>](https://huggingface.co/thu-coai/CDial-GPT_LCCC-base)  | 95.5M | [LCCC-base](##datasets)  | A GPT model post-trained on LCCC-base dataset from GPT<sub>Novel</sub> |
| [CDial-GPT2<sub>LCCC-base</sub>](https://huggingface.co/thu-coai/CDial-GPT2_LCCC-base) | 95.5M | [LCCC-base](##datasets)  | A GPT2 model post-trained on LCCC-base dataset from GPT<sub>Novel</sub> |
| [CDial-GPT<sub>LCCC-large</sub>](https://huggingface.co/thu-coai/CDial-GPT_LCCC-large) | 95.5M | [LCCC-large](##datasets) | A GPT model post-trained on LCCC-large dataset from GPT<sub>Novel</sub> |

### Installation  
Install from the source codes:

    git clone https://github.com/thu-coai/CDial-GPT.git
    cd CDial-GPT
    pip install -r requirements.txt 
    
### Quick Start
Step 1: Prepare the data for fine-tuning (E.g., [STC dataset](https://arxiv.org/abs/1503.02364) or "data/toy_data.json" in our respository) and the pre-trianed model:
    
    wget https://cloud.tsinghua.edu.cn/f/372be4a9994b4124810e/?dl=1 -O STC-corpus.zip  # Download the STC dataset and unzip into "data_path" dir (fine-tuning on STC)
    git lfs install
    git clone https://huggingface.co/thu-coai/CDial-GPT_LCCC-large  # or OpenAIGPTLMHeadModel.from_pretrained("thu-coai/CDial-GPT_LCCC-large")
  
Step 2: Train the model

    python train.py --pretrained --model_checkpoint thu-coai/CDial-GPT_LCCC-large --data_path data/STC.json --scheduler linear  # Single GPU training

or

    python -m torch.distributed.launch --nproc_per_node=8 train.py --pretrained --model_checkpoint thu-coai/CDial-GPT_LCCC-large --data_path data/STC.json --scheduler linear  # Training on 8 GPUs

Note: We have also provided ``train_path`` argument in the training script to read dataset in plain text, which will be sliced and handled distributionally.
You can consider to use this argument if the dataset is too large for your system's memory. (also, remember to leave the ``data_path`` argument empty if you are using ``train_path``). 

Step 3: Inference mode

    # YOUR_MODEL_PATH: the model path used for generation
    python infer.py --model_checkpoint YOUR_MODEL_PATH --datapath data/STC_test.json --out_path STC_result.txt  # Do Inference on a corpus
    python interact.py --model_checkpoint YOUR_MODEL_PATH  # Interact on the terminal

Training Arguments

| Arguments  | Type     | Default value  | Description |
| :---- | :---------- | :----- | :------- |
| model_checkpoint | str | "" | Path or URL of model files (Directory of pre-training model and config/vocab files) |
| pretrained  | bool | False | If False, then train the model from scratch |
| data_path | str | "" | Path of the dataset |
| dataset_cache | str | default="dataset_cache" | Path or url of the dataset cache |
| train_path | str | "" | Path of the training set for distributed dataset |
| valid_path | str | "" | Path of the validation set for distributed dataset |
| log_file | str | "" | Output logs to a file under this path |
| num_workers | int | 1 | Number of subprocesses for data loading |
| n_epochs | int | 70 | Number of training epochs |
| train_batch_size | int | 8 | Batch size for training |
| valid_batch_size | int | 8 | Batch size for validation |
| max_history | int | 15 | Number of previous exchanges to keep in history |
| scheduler | str | "noam" | Method of optimizer |
| n_emd | int | 768 | Number of n_emd in config file (for noam) |
| eval_before_start | bool | False | If true, start evaluation before training |
| warmup_steps | int | 5000 | Warm up steps |
| valid_steps | int | 0 | Perform validation every X steps, if is not 0 |
| gradient_accumulation_steps | int | 64 | Accumulate gradients on several steps |
| max_norm | float | 1.0 | Clipping gradient norm |
| device | str | "cuda" if torch.cuda.is_available() else "cpu" | Device (cuda or cpu) |
| fp16 | str | "" | Set to O0, O1, O2 or O3 for fp16 training (see apex documentation) |
| local_rank | int | -1 | Local rank for distributed training (-1: not distributed) |

## <a name="#Evaluation-en">Evaluation</a> 
Evaluation is performed on results generated by models fine-tuned on
[STC dataset](https://cloud.tsinghua.edu.cn/f/372be4a9994b4124810e/).
All responses are generated using the [Nucleus Sampling](https://arxiv.org/abs/1904.09751) scheme with a threshold 0.9 and temperature 0.7.

#### Automatic Evaluation

| Models  | Model Size | PPL  | BLEU-2 | BLEU-4 | Dist-1 | Dist-2 | Greedy Matching | Embedding Average |
| :------ | :------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: |
| Attn-Seq2seq | 73M | 34.20 | 3.93 | 0.90 | 8.5 | 11.91 | 65.84 | 83.38 |
| Transformer | 113M | 22.10 | 6.72 | 3.14 | 8.8 | 13.97 | 66.06 | 83.55 |
| GPT2-chitchat | 88M | - | 2.28 | 0.54 | 10.3 | 16.25 | 61.54 | 78.94 |
| GPT<sub>Novel</sub> | 95.5M | 21.27 | 5.96 | 2.71 | 8.0 | 11.72 | 66.12 | 83.34 |
| GPT<sub>LCCC-base</sub> | 95.5M | 18.38 | 6.48 | 3.08 | 8.3 | 12.68 | 66.21 | 83.54 |
| GPT2<sub>LCCC-base</sub> | 95.5M | 22.76 | 5.69 | 2.50 | 7.7 | 10.87 | 66.24 | 83.46 |
| GPT<sub>LCCC-large</sub> | 95.5M | 18.23 | 6.63 | 3.20 | 8.3 | 12.71 | 66.25 | 83.63 |

#### Human Evaluation

We also employed 3 crowd-sourced annotators to evaluate 200 samples of each model about fluency, relevance, and diversity based on a 2/1/0 score schema. Refer to our [paper](https://arxiv.org/abs/2008.03946) for more details.

| Models  | +2      | +1      | +0      | Kappa   |
| :----- | :-----: | :-----: | :-----: | :-----: |
| Attn-Seq2Seq | 27.1% | 21.4% | 51.4% | 0.4544 |
| Transformer | 42.4% | 23.6% | 34.0% | 0.4429 |
| GPT2-chitchat | 24.3% | 19,1% | 56.6% | 0.3941 |
| GPT<sub>LCCC-base</sub> | 46.7% | 21.8% | 31.5% | 0.3954 |
| GPT<sub>LCCC-large</sub> | 48.6% | 24.5% | 27.0% | 0.4203 |

#### Toxic Generation Statistics

We randomly select 1000 generated samples, and make a statistic of toxic generation:

| Repetition | Dirty word | Other Bad Cases |
| :----------: | :-----: | :-------: |
| 23 | 9 | 2 |

## Case study of GPT<sub>LCCC-large</sub>
Single-turn generated examples from STC dataset (fine-tuned on STC):

| Post | Response |
| :----- | :---------- |
| è¿™ å±… ç„¶ æ˜¯ åŒ— äº¬ ä¸ æ˜¯ å“ˆ å°” æ»¨ ã€‚ | å“ˆ å°” æ»¨ çš„ å¤© æ°” å¥½ åƒ æ¯” åŒ— äº¬ å¥½ ç‚¹ ï¼Œ æˆ‘ è¿™ é‡Œ å°± æ˜¯ å° é›¨ ï¼Œ ç™½ å¤© éƒ½ æ˜¯ å¤§ é£ |
| è¿ è¥ å•† æ ¡ å›­ å¹¿ å‘Š å¤§ P K ï¼Œ å¤ª æ¬¢ ä¹ äº† ï¼ å“ˆ å“ˆ å“ˆ ã€‚ | æˆ‘ ä»¥ ä¸º æ˜¯ é©¬ äº‘ çš„ å¹¿ å‘Š ã€‚ |
| D e l s a S p o s a ç³– æœ è‰² ç³» åˆ— å©š çº± ï¼Œ ç²‰ è“ çº¢ ç´« ï¼Œ ä¿ çš® åˆ æ¸… æ–° ï¼Œ ä½  å–œ æ¬¢ å— ï¼Ÿ | æœ€ å ä¸€ ä»¶ å¤ª ç¾ äº† ï¼Œ ç¾ çš„ ä¸ åƒ è¯ ï¼ |

Multi-turn generated examples using the interactive script:

| Roel | Response |
| :----- | :---------- |
| User | åˆ«çˆ±æˆ‘æ²¡ç»“æœ |
| Bot | æ²¡ ç»“ æœ çš„ çˆ± æ˜¯ å¤š ä¹ˆ çš„ ç—› ã€‚ |
| User | ä½†æ˜¯çˆ±è¿‡å°±å¯ä»¥äº† |
| Bot | æœ‰ æ—¶ å€™ ï¼Œçˆ± æƒ… å°± æ˜¯ ä¸€ åœº æ¸¸ æˆ ã€‚ |
| User | è¾“çš„æ€»æ˜¯æˆ‘ |
| Bot | æˆ‘ çˆ± ä½  ï¼Œ å°± åƒ æˆ‘ çˆ± ä½  ä¸€ æ · ã€‚ |
| User | ä½ å¯æ‹‰åˆ°å§ |
| Bot | ä½  å¯ æ‹‰ åˆ° å§ ã€‚ |
| User | åˆ«å­¦æˆ‘è¯´è¯ |
| Bot | æˆ‘ è¯´ è¯ å°± æ˜¯ ä½  ä¸ è¡Œ ã€‚ |
| User | é€¼æˆ‘åŠ¨æ‰‹æ˜¯å§ |
| Bot | ä¸ ï¼Œ æ˜¯ é€¼ ä½  åŠ¨ å£ æ˜¯ å§ |
 
[//]: #  (Multi-turn self-playing dialogue with user prompt:)

[//]: #  (Commonsense question answering examples:)

## Authors
[Yida Wang](https://github.com/lemon234071), [Pei Ke](https://github.com/kepei1106), [Yinhe Zheng](https://github.com/silverriver), [Kaili Huang](https://github.com/KellyHuang63), Yong Jiang, Xiaoyan Zhu, Minlie Huang

## Acknowledgement
[Zuoxian Ye](https://github.com/yezuoxian), Yao Wang, Yifan Pan

## Disclaimer
The LCCC dataset and the pre-trained models aim to facilitate the research for conversation generation. The LCCC dataset provided in this repository originates from various sources. Although a rigorous cleaning process has been carried out, there is no guarantee that all the inappropriate contents have been completely filtered out. All the contents contained in this dataset does not represent the authors' opinion.
This repository contains only part of the modeling machinery needed to actually produce a dialogue model. The decoding script provided in this repository is only for the research purpose. We are not responsible for any contents generated using our model.

## Citation
Please kindly cite our [paper](https://arxiv.org/abs/2008.03946) if you use the datasets or models in your research:

    @inproceedings{wang2020chinese,
      title={A Large-Scale Chinese Short-Text Conversation Dataset},
      author={Wang, Yida and Ke, Pei and Zheng, Yinhe and Huang, Kaili and Jiang, Yong and Zhu, Xiaoyan and Huang, Minlie},
      booktitle={NLPCC},
      year={2020},
      url={https://arxiv.org/abs/2008.03946}
    }
